{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1ppglemgrwuhilEp9Q8FSZhuDcYLES3kH",
      "authorship_tag": "ABX9TyPqraeM97M6NrXRJ7SBLPXb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rakibulhaque9954/Machine_Learning_Translation/blob/main/machine_translation_with_rnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "6OyNWaIhGugO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "1vu7wTWKGfMI"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf### models\n",
        "import numpy as np### math computations\n",
        "import matplotlib.pyplot as plt### plotting bar chart\n",
        "import sklearn### machine learning library\n",
        "import cv2## image processing\n",
        "from sklearn.metrics import confusion_matrix, roc_curve### metrics\n",
        "import seaborn as sns### visualizations\n",
        "import datetime\n",
        "import pathlib\n",
        "import io\n",
        "import os\n",
        "import re\n",
        "import string\n",
        "import time\n",
        "from numpy import random\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow_probability as tfp\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Layer\n",
        "from tensorflow.keras.layers import (Dense,Flatten,SimpleRNN,InputLayer,Conv1D,Bidirectional,GRU,LSTM,BatchNormalization,Dropout,Input, Embedding,TextVectorization)\n",
        "from tensorflow.keras.losses import BinaryCrossentropy,CategoricalCrossentropy, SparseCategoricalCrossentropy\n",
        "from tensorflow.keras.metrics import Accuracy,TopKCategoricalAccuracy, CategoricalAccuracy, SparseCategoricalAccuracy\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from google.colab import drive\n",
        "from google.colab import files\n",
        "from tensorboard.plugins import projector"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset Prepration and Preprocessing"
      ],
      "metadata": {
        "id": "f0k-Dr2vHsLF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset Download"
      ],
      "metadata": {
        "id": "XtJG-aLgHmew"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Small dataset manythings.org"
      ],
      "metadata": {
        "id": "iDjk-thfImBK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://www.manythings.org/anki/fra-eng.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RJi1bKw2HqVg",
        "outputId": "47204a73-963f-499a-a295-d46182266436"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-10-21 11:49:30--  https://www.manythings.org/anki/fra-eng.zip\n",
            "Resolving www.manythings.org (www.manythings.org)... 173.254.30.110\n",
            "Connecting to www.manythings.org (www.manythings.org)|173.254.30.110|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 7757635 (7.4M) [application/zip]\n",
            "Saving to: ‘fra-eng.zip’\n",
            "\n",
            "fra-eng.zip         100%[===================>]   7.40M  3.88MB/s    in 1.9s    \n",
            "\n",
            "2023-10-21 11:49:33 (3.88 MB/s) - ‘fra-eng.zip’ saved [7757635/7757635]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip '/content/fra-eng.zip' -d '/content/dataset/'\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GxCUnBhPHylV",
        "outputId": "06e7c8c7-89cc-40ec-c943-34cf6ddcb658"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/fra-eng.zip\n",
            "  inflating: /content/dataset/_about.txt  \n",
            "  inflating: /content/dataset/fra.txt  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Kaggle Dataset"
      ],
      "metadata": {
        "id": "wl2SNEvEIttI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q kaggle\n",
        "!mkdir ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 /root/.kaggle/kaggle.json\n",
        "!kaggle datasets download -d dhruvildave/en-fr-translation-dataset\n",
        "!unzip \"/content/en-fr-translation-dataset.zip\" -d \"/content/dataset/\""
      ],
      "metadata": {
        "id": "VhKQqC8fIwBj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = tf.data.experimental.CsvDataset(\n",
        "  \"/content/dataset/en-fr.csv\",\n",
        "  [\n",
        "    tf.string,\n",
        "    tf.string\n",
        "  ],\n",
        ")"
      ],
      "metadata": {
        "id": "N_F6TswdI3fC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Processing"
      ],
      "metadata": {
        "id": "OQFSFupRITO1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_dataset = tf.data.TextLineDataset('/content/dataset/fra.txt')\n",
        "\n"
      ],
      "metadata": {
        "id": "_KlQNKZ_H5ur"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in text_dataset.take(5):\n",
        "  print(i)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UMemm6h3JFUY",
        "outputId": "f27653a5-3621-4b50-b933-101285e1f6fe"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(b'Go.\\tVa !\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2877272 (CM) & #1158250 (Wittydev)', shape=(), dtype=string)\n",
            "tf.Tensor(b'Go.\\tMarche.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2877272 (CM) & #8090732 (Micsmithel)', shape=(), dtype=string)\n",
            "tf.Tensor(b'Go.\\tEn route !\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2877272 (CM) & #8267435 (felix63)', shape=(), dtype=string)\n",
            "tf.Tensor(b'Go.\\tBouge !\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2877272 (CM) & #9022935 (Micsmithel)', shape=(), dtype=string)\n",
            "tf.Tensor(b'Hi.\\tSalut !\\tCC-BY 2.0 (France) Attribution: tatoeba.org #538123 (CM) & #509819 (Aiji)', shape=(), dtype=string)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "VOCAB_SIZE = 20000\n",
        "ENGLISH_SEQUENCE_LENGTH = 64 # explicitly initialising input and output length of sequences\n",
        "FRENCH_SEQUENCE_LENGTH = 64\n",
        "EMBEDDING_DIM = 300\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "zF9jnVObJHK2"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "english_vectorize_layer = tf.keras.layers.TextVectorization(\n",
        "    standardize='lower_and_strip_punctuation',\n",
        "    max_tokens=VOCAB_SIZE,\n",
        "    output_mode='int',\n",
        "    output_sequence_length=ENGLISH_SEQUENCE_LENGTH)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "0pa62-K5Jrez"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "french_vectorize_layer = tf.keras.layers.TextVectorization(\n",
        "    standardize='lower_and_strip_punctuation',\n",
        "    max_tokens=VOCAB_SIZE,\n",
        "    output_mode='int',\n",
        "    output_sequence_length=FRENCH_SEQUENCE_LENGTH)"
      ],
      "metadata": {
        "id": "ODuIx_zdJ46t"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def selector(input_text):\n",
        "  \"\"\"[starttoken] and [endtoken] token initialized just as the model explanation the [endtoken] part is the output\"\"\"\n",
        "  split_text = tf.strings.split(input_text, sep='\\t')\n",
        "  return {'input_1' : split_text[0 : 1], 'input_2' : '[starttoken] '+ split_text[1 : 2]}, split_text[1 : 2] + ' [endtoken]'\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "LfkhF2FpKmhH"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "split_dataset = text_dataset.map(selector)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "nAIB7luDK0KB"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in split_dataset.take(1):\n",
        "  print(i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HM6is1tjK4n6",
        "outputId": "1471476c-414b-4cfc-9fca-6538dba9dfbf"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "({'input_1': <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'Go.'], dtype=object)>, 'input_2': <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'[starttoken] Va !'], dtype=object)>}, <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'Va ! [endtoken]'], dtype=object)>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**As the data shows how the inputs have been seperated:**\n",
        "- Input 1 - {'input_1': <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'Go.']>\n",
        "- Input 2 - 'input_2': <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'[starttoken] Va !']>\n",
        "- Output - <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'Va ! [endtoken]']>\n",
        "\n",
        "***The inputs and output has been processed for training, the split dataset is for training while init is for vocabulary creation.***"
      ],
      "metadata": {
        "id": "M44tqL0zHtpQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def selector(input_text):\n",
        "  \"\"\"This function is needed to create vocabulary\"\"\"\n",
        "  split_text = tf.strings.split(input_text, sep='\\t')\n",
        "  return split_text[0 : 1], '[starttoken] ' + split_text[1 : 2] + ' [endtoken]' # combining input 2 and output the decoder part"
      ],
      "metadata": {
        "id": "mrZuK9PVJTIn"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "init_dataset = text_dataset.map(selector)\n"
      ],
      "metadata": {
        "id": "lDbk7XHpKfgi"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in init_dataset.take(5):\n",
        "  print(i)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fdilxCQ-Kx9Q",
        "outputId": "0767fccd-f603-44f4-e3ea-e9c1d578f19d"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(<tf.Tensor: shape=(1,), dtype=string, numpy=array([b'Go.'], dtype=object)>, <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'[starttoken] Va ! [endtoken]'], dtype=object)>)\n",
            "(<tf.Tensor: shape=(1,), dtype=string, numpy=array([b'Go.'], dtype=object)>, <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'[starttoken] Marche. [endtoken]'], dtype=object)>)\n",
            "(<tf.Tensor: shape=(1,), dtype=string, numpy=array([b'Go.'], dtype=object)>, <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'[starttoken] En route ! [endtoken]'], dtype=object)>)\n",
            "(<tf.Tensor: shape=(1,), dtype=string, numpy=array([b'Go.'], dtype=object)>, <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'[starttoken] Bouge ! [endtoken]'], dtype=object)>)\n",
            "(<tf.Tensor: shape=(1,), dtype=string, numpy=array([b'Hi.'], dtype=object)>, <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'[starttoken] Salut ! [endtoken]'], dtype=object)>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "english_training_data = init_dataset.map(lambda x,y: x) # x is for eng\n",
        "english_vectorize_layer.adapt(english_training_data)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Ynnm-FkcJ6cq"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "french_training_data = init_dataset.map(lambda x,y: y) # y is for french\n",
        "french_vectorize_layer.adapt(french_training_data)"
      ],
      "metadata": {
        "id": "BCDZdOzbKJPr"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def vectorizer(inputs, output):\n",
        "  return {'input_1' : english_vectorize_layer(inputs['input_1']),\n",
        "          'input_2' : french_vectorize_layer(inputs['input_2']) }, french_vectorize_layer(output)\n",
        "\n",
        "# making the input_1 and input_2 together for inputs\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Ti2mP6FeLH36"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = split_dataset.map(vectorizer)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ABzkpFwhMsEF"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in dataset.take(1):\n",
        "  print(i)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ELPzW9n9NzgI",
        "outputId": "6deb6b61-08da-44a0-db3b-aac9e3dbd3d9"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "({'input_1': <tf.Tensor: shape=(1, 64), dtype=int64, numpy=\n",
            "array([[44,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]])>, 'input_2': <tf.Tensor: shape=(1, 64), dtype=int64, numpy=\n",
            "array([[  2, 103,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0]])>}, <tf.Tensor: shape=(1, 64), dtype=int64, numpy=\n",
            "array([[103,   3,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0]])>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'ENG: {english_vectorize_layer.get_vocabulary()[44]}')\n",
        "print(f'FR: {french_vectorize_layer.get_vocabulary()[2]}')\n",
        "print(f'FR: {french_vectorize_layer.get_vocabulary()[103]}')\n",
        "print(f'FR: {french_vectorize_layer.get_vocabulary()[3]}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Itq8ND5WMt7G",
        "outputId": "c4236b66-009d-4c0e-b8f5-723516948722"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ENG: go\n",
            "FR: starttoken\n",
            "FR: va\n",
            "FR: endtoken\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modeling"
      ],
      "metadata": {
        "id": "D1wPhN-3539y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Seq2Seq Model\n",
        "<h4>Model Diagram<h4>\n",
        "<img src='https://miro.medium.com/max/942/1*KtWwvLK-jpGPSnj3tStg-Q.png'>"
      ],
      "metadata": {
        "id": "jZcFVwncRTS7"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7UuJlj0FMxEg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}