{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rakibulhaque9954/Machine_Learning_Translation/blob/main/Machine_translation_Transformers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VGgjjqp2_ZzA"
      },
      "source": [
        "# Acknowledgement"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ojkq8lGs_ezj"
      },
      "source": [
        "**Based on research by members of Google Brain, Google Research, Univerity of Toronto**<br>\n",
        "Paper Link: https://arxiv.org/pdf/1706.03762.pdf\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YkeasEue5qIo"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Vt93_FwG0cLc"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf### models\n",
        "import numpy as np### math computations\n",
        "import matplotlib.pyplot as plt### plotting bar chart\n",
        "import sklearn### machine learning library\n",
        "import cv2## image processing\n",
        "from sklearn.metrics import confusion_matrix, roc_curve### metrics\n",
        "import seaborn as sns### visualizations\n",
        "import datetime\n",
        "import pathlib\n",
        "import io\n",
        "import os\n",
        "import re\n",
        "import string\n",
        "import time\n",
        "from numpy import random\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow_probability as tfp\n",
        "from keras.models import Model\n",
        "from keras.layers import Layer\n",
        "from keras.layers import (Dense,Flatten,SimpleRNN,InputLayer,Conv1D,Bidirectional,GRU,LSTM,BatchNormalization,Dropout,Input, Embedding,TextVectorization)\n",
        "from keras.losses import BinaryCrossentropy,CategoricalCrossentropy, SparseCategoricalCrossentropy\n",
        "from keras.metrics import Accuracy,TopKCategoricalAccuracy, CategoricalAccuracy, SparseCategoricalAccuracy\n",
        "from keras.optimizers import Adam\n",
        "from keras.layers import MultiHeadAttention, LayerNormalization\n",
        "from google.colab import drive\n",
        "from google.colab import files\n",
        "from tensorboard.plugins import projector"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yHoYbLZ0Vd1Z"
      },
      "source": [
        "# Data Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pHsvMYjUVhzT"
      },
      "source": [
        "## Dataset Download"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ntrhZXbqVhU0",
        "outputId": "54dbd0b0-2cbf-4e3b-8850-4a5c305d1ab2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-10-27 09:53:51--  https://www.manythings.org/anki/fra-eng.zip\n",
            "Resolving www.manythings.org (www.manythings.org)... 173.254.30.110\n",
            "Connecting to www.manythings.org (www.manythings.org)|173.254.30.110|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 7757635 (7.4M) [application/zip]\n",
            "Saving to: ‘fra-eng.zip’\n",
            "\n",
            "fra-eng.zip         100%[===================>]   7.40M  5.62MB/s    in 1.3s    \n",
            "\n",
            "2023-10-27 09:53:53 (5.62 MB/s) - ‘fra-eng.zip’ saved [7757635/7757635]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://www.manythings.org/anki/fra-eng.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ogDgWojVmgW",
        "outputId": "45c3d31b-524b-47a9-8bac-131e906fdeeb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/fra-eng.zip\n",
            "  inflating: /content/dataset/_about.txt  \n",
            "  inflating: /content/dataset/fra.txt  \n"
          ]
        }
      ],
      "source": [
        "!unzip \"/content/fra-eng.zip\" -d \"/content/dataset/\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nVKzR7bY19KK"
      },
      "source": [
        "## Kaggle Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "QNSRkIC72X9d"
      },
      "outputs": [],
      "source": [
        "# !pip install -q kaggle\n",
        "# !mkdir ~/.kaggle\n",
        "# !cp kaggle.json ~/.kaggle/\n",
        "# !chmod 600 /root/.kaggle/kaggle.json\n",
        "# !kaggle datasets download -d dhruvildave/en-fr-translation-dataset\n",
        "# !unzip \"/content/en-fr-translation-dataset.zip\" -d \"/content/dataset/\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "XEZz30Ux2eeS"
      },
      "outputs": [],
      "source": [
        "# dataset = tf.data.experimental.CsvDataset(\n",
        "#     \"/content/dataset/en-fr.csv\",\n",
        "#     [tf.string, tf.string],\n",
        "#     header=True  # Assumes the CSV file has a header\n",
        "# )\n",
        "\n",
        "# !mkdir /content/dataset1\n",
        "\n",
        "# with open(\"/content/dataset/en-fr.txt\", \"w\") as f:\n",
        "#     for english, french in dataset:\n",
        "#         # Convert TensorFlow tensors to Python strings\n",
        "#         english_str = english.numpy().decode(\"utf-8\")\n",
        "#         french_str = french.numpy().decode(\"utf-8\")\n",
        "\n",
        "#         # Write to the text file, separating the two languages by a tab character\n",
        "#         f.write(f\"{english_str}\\t{french_str}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "zqJzDztr2zzg"
      },
      "outputs": [],
      "source": [
        "# for i in dataset.take(3):\n",
        "#   print(i)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "496hge3MVqfy"
      },
      "source": [
        "## Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "8PTxBVhRVtHn"
      },
      "outputs": [],
      "source": [
        "text_dataset = tf.data.TextLineDataset(\"/content/dataset/fra.txt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I3J-7rGF2xGx",
        "outputId": "b664a2f1-2eff-4c2f-f6b2-c436f28b3b37"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(b'Go.\\tVa !\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2877272 (CM) & #1158250 (Wittydev)', shape=(), dtype=string)\n",
            "tf.Tensor(b'Go.\\tMarche.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2877272 (CM) & #8090732 (Micsmithel)', shape=(), dtype=string)\n",
            "tf.Tensor(b'Go.\\tEn route !\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2877272 (CM) & #8267435 (felix63)', shape=(), dtype=string)\n"
          ]
        }
      ],
      "source": [
        "for i in text_dataset.take(3):\n",
        "  print(i)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "BdHMIIBUVwk1"
      },
      "outputs": [],
      "source": [
        "VOCAB_SIZE = 20000\n",
        "ENGLISH_SEQUENCE_LENGTH = 32\n",
        "FRENCH_SEQUENCE_LENGTH = 32\n",
        "EMBEDDING_DIM = 512\n",
        "BATCH_SIZE = 128"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "FWKfvKc0VyZP"
      },
      "outputs": [],
      "source": [
        "english_vectorize_layer = TextVectorization(\n",
        "    standardize='lower_and_strip_punctuation',\n",
        "    max_tokens=VOCAB_SIZE,\n",
        "    output_mode='int',\n",
        "    output_sequence_length=ENGLISH_SEQUENCE_LENGTH\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "1Pxg86h3Vz4T"
      },
      "outputs": [],
      "source": [
        "french_vectorize_layer = TextVectorization(\n",
        "    standardize='lower_and_strip_punctuation',\n",
        "    max_tokens=VOCAB_SIZE,\n",
        "    output_mode='int',\n",
        "    output_sequence_length=FRENCH_SEQUENCE_LENGTH\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "cuF4bBn0V1wG"
      },
      "outputs": [],
      "source": [
        "def selector(input_text):\n",
        "  split_text = tf.strings.split(input_text,'\\t')\n",
        "  return {'input_1':split_text[0:1],'input_2':'starttoken '+split_text[1:2]},split_text[1:2]+' endtoken'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "XyqsU18hV2hl"
      },
      "outputs": [],
      "source": [
        "split_dataset = text_dataset.map(selector)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "qCv6PgKNV35i"
      },
      "outputs": [],
      "source": [
        "def separator(input_text):\n",
        "  split_text = tf.strings.split(input_text,'\\t')\n",
        "  return split_text[0:1],'starttoken '+split_text[1:2]+' endtoken'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "JrPDAYWZV6Uh"
      },
      "outputs": [],
      "source": [
        "init_dataset = text_dataset.map(separator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ju_kmPLLV7yR",
        "outputId": "57940ab4-0e5c-4af1-d030-58763d337d29"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "({'input_1': <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'Go.'], dtype=object)>, 'input_2': <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'starttoken Va !'], dtype=object)>}, <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'Va ! endtoken'], dtype=object)>)\n",
            "({'input_1': <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'Go.'], dtype=object)>, 'input_2': <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'starttoken Marche.'], dtype=object)>}, <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'Marche. endtoken'], dtype=object)>)\n",
            "({'input_1': <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'Go.'], dtype=object)>, 'input_2': <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'starttoken En route !'], dtype=object)>}, <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'En route ! endtoken'], dtype=object)>)\n"
          ]
        }
      ],
      "source": [
        "for i in split_dataset.take(3):\n",
        "  print(i)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Poh42v6PV-Q3"
      },
      "source": [
        "### Vocabulary Creation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "foghUS5HWA-3"
      },
      "outputs": [],
      "source": [
        "english_training_data=init_dataset.map(lambda x,y:x) # input x,y and output x\n",
        "english_vectorize_layer.adapt(english_training_data) # adapt the vectorize_layer to the training data\n",
        "\n",
        "french_training_data=init_dataset.map(lambda x,y:y) # input x,y,z and output y\n",
        "french_vectorize_layer.adapt(french_training_data) # adapt the vectorize_layer to the training data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K57EhISUWIOZ"
      },
      "source": [
        "### Grouping and Vectorizing for training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "HnCwIX7hWG3J"
      },
      "outputs": [],
      "source": [
        "def vectorizer(inputs,output):\n",
        "  return {'input_1':english_vectorize_layer(inputs['input_1']),\n",
        "          'input_2':french_vectorize_layer(inputs['input_2'])},french_vectorize_layer(output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "M8iZo8VWWNvw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2a3d0c7-5e45-4e2f-f8f0-c488391bfc9f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<_MapDataset element_spec=({'input_1': TensorSpec(shape=(None,), dtype=tf.string, name=None), 'input_2': TensorSpec(shape=(None,), dtype=tf.string, name=None)}, TensorSpec(shape=(None,), dtype=tf.string, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ],
      "source": [
        "split_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "AbOPWonzWPxb"
      },
      "outputs": [],
      "source": [
        "dataset=split_dataset.map(vectorizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "SZ9c340xWRh7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a28fdcb2-325f-43f7-9c3a-7c0d74dc8940"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "({'input_1': <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'Go.'], dtype=object)>, 'input_2': <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'starttoken Va !'], dtype=object)>}, <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'Va ! endtoken'], dtype=object)>)\n",
            "({'input_1': <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'Go.'], dtype=object)>, 'input_2': <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'starttoken Marche.'], dtype=object)>}, <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'Marche. endtoken'], dtype=object)>)\n",
            "({'input_1': <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'Go.'], dtype=object)>, 'input_2': <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'starttoken En route !'], dtype=object)>}, <tf.Tensor: shape=(1,), dtype=string, numpy=array([b'En route ! endtoken'], dtype=object)>)\n"
          ]
        }
      ],
      "source": [
        "for i in split_dataset.take(3):\n",
        "  print(i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "ObbndISGWSCO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad8578e5-bc84-4ab5-e12d-4c6082d36595"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "({'input_1': <tf.Tensor: shape=(1, 32), dtype=int64, numpy=\n",
            "array([[44,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]])>, 'input_2': <tf.Tensor: shape=(1, 32), dtype=int64, numpy=\n",
            "array([[  2, 103,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "          0,   0,   0,   0,   0,   0]])>}, <tf.Tensor: shape=(1, 32), dtype=int64, numpy=\n",
            "array([[103,   3,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "          0,   0,   0,   0,   0,   0]])>)\n"
          ]
        }
      ],
      "source": [
        "for i in dataset.take(1):\n",
        "  print(i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "_HegZ8trWVtu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5f029a3-25dd-49ec-a74a-77bd34bcc96f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<_MapDataset element_spec=({'input_1': TensorSpec(shape=(None, 32), dtype=tf.int64, name=None), 'input_2': TensorSpec(shape=(None, 32), dtype=tf.int64, name=None)}, TensorSpec(shape=(None, 32), dtype=tf.int64, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ],
      "source": [
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "lf8z62zYWXtt"
      },
      "outputs": [],
      "source": [
        "dataset = dataset.shuffle(2048).unbatch().batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "UmcfGjPHWjRW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "725fc8c5-b754-4e9a-9347-752df350993f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<_PrefetchDataset element_spec=({'input_1': TensorSpec(shape=(None, 32), dtype=tf.int64, name=None), 'input_2': TensorSpec(shape=(None, 32), dtype=tf.int64, name=None)}, TensorSpec(shape=(None, 32), dtype=tf.int64, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ],
      "source": [
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "CC75EqgIWlgD"
      },
      "outputs": [],
      "source": [
        "NUM_BATCHES = int(200000/BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h4W-wmztWnpS"
      },
      "source": [
        "### Dataset Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "sE3NStdoWm9G"
      },
      "outputs": [],
      "source": [
        "train_dataset = dataset.take(int(0.9*NUM_BATCHES))\n",
        "val_dataset = dataset.skip(int(0.9*NUM_BATCHES))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "KtyT8GVvWrgL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a03b6629-ae27-4402-ac0d-a3869e42d641"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<_TakeDataset element_spec=({'input_1': TensorSpec(shape=(None, 32), dtype=tf.int64, name=None), 'input_2': TensorSpec(shape=(None, 32), dtype=tf.int64, name=None)}, TensorSpec(shape=(None, 32), dtype=tf.int64, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ],
      "source": [
        "train_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xrvBxtH4XDVc"
      },
      "source": [
        "# Modeling\n",
        "\n",
        "<hr>\n",
        "<h4>Model Architecture</h4>\n",
        "<hr>\n",
        "<img src='https://machinelearningmastery.com/wp-content/uploads/2021/08/attention_research_1.png'>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d65NBU1S1Vde"
      },
      "source": [
        "***Step Wise Explanation:***\n",
        "- Input Embedding: The process begins with encoding the input language (e.g., English sequence) into numerical vectors. Each word or token is transformed into a high-dimensional vector.\n",
        "- Multi-Head Self-Attention: This is the heart of a transformer. The model looks at each word in the input sentence and assigns different levels of importance to other words in the sentence. Multiple attention heads allow the model to focus on different aspects of the sentence simultaneously.\n",
        "- Positional Encoding: Since transformers don't have an inherent sense of word order, positional encoding is added to the word embeddings to help the model understand the word's position in the sentence.\n",
        "- Encoder-Decoder Architecture: In translation tasks, there are typically two parts: the encoder and the decoder. The encoder takes the input sentence and processes it, while the decoder generates the translated output.\n",
        "- Decoder Self-Attention: The decoder also uses multi-head self-attention, but it's slightly modified to prevent it from looking ahead in the output sentence, which would result in incorrect translations.\n",
        "- Attention Output: The outputs from the attention mechanisms are used to calculate attention scores, which determine how much each word in the input sentence contributes to each word in the output sentence.\n",
        "Position-wise Feedforward Networks: After attention, the model passes the data through feedforward neural networks to further process and refine the information.\n",
        "Output Layer: The final layer in the decoder produces probabilities for each word in the target language vocabulary, allowing the model to predict the next word in the translation.\n",
        "- Training and Optimization: Transformers are trained using large parallel corpora of source and target language sentences. They learn to minimize the difference between predicted translations and the actual translations in the training data.\n",
        "- Repeat for Each Token: This process is repeated for each word in the output sentence, where the previously generated words are used as context for generating the next word.\n",
        "Beam Search or Greedy Decoding: During inference, the model generates translations one word at a time. Beam search or greedy decoding is often used to select the most likely next word based on the model's predictions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82vlVMX1QsJM"
      },
      "source": [
        "<h4>Inside Attention Layer</h4>\n",
        "<img src='https://production-media.paperswithcode.com/methods/35184258-10f5-4cd0-8de3-bd9bc8f88dc3.png'>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OcBq72NyXBgO"
      },
      "source": [
        "Easy to understand Explanation:\n",
        "\n",
        "Lets break it down and relate it to the components and processes in a transformer model:\n",
        "\n",
        "- School and Students: Think of the school as the entire context, and the students as the individual tokens in a sequence.\n",
        "- Vectorization and Tokenization: The process of converting students into tokens and vectorizing them represents the initial preprocessing steps where text data is tokenized into individual words or tokens and then converted into numerical vector representations.\n",
        "- Vocabulary: The vocabulary of the school represents the set of unique tokens (students) that the model has learned from various schools within the same company. These tokens are used to represent words in the sequences.\n",
        "- Intra-Attention (Self-Attention):\n",
        "Each student's interaction with their classmates represents the intra-attention mechanism, where relationships, influences, and context between tokens (students) are captured.\n",
        "Each student becomes a query (Q), and their classmates become keys (K) and values (V).\n",
        "Attention scores are calculated to determine how much weight each student should give to their classmates.\n",
        "Softmax normalization of attention scores can be thought of as grading each student's relationships and influence on others.\n",
        "Concatenation of information from different teachers (heads) captures diverse insights.\n",
        "- Inter-Attention (Cross-Attention):\n",
        "When different classes (decoders) want to compare their students (tokens), it's akin to cross-attention between different parts of the model.\n",
        "A student from one class becomes a query (Q), and the students from another class become keys (K) and values (V).\n",
        "The process is similar to intra-attention but operates across different classes.\n",
        "- Linear Layer: The linear layer represents the post-attention processing step that helps combine and refine information before producing the final output.\n",
        "\n",
        "This is the essence of how attention mechanisms work in transformers, where tokens (students) attend to each other, calculate their influence, and produce context vectors (mark sheets) for each other. These context vectors are then used in cross-attention to compare tokens from different parts of the model, ultimately leading to the model's final output."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pTCpHdYylY9t"
      },
      "source": [
        "Encoder's Role (Intra-Attention in Encoder):\n",
        "The encoder processes the input sequence and performs intra-attention.\n",
        "It produces context vectors (contextual representations) for each word in the input sequence.\n",
        "These context vectors capture information about how each word relates to others within the input sequence.\n",
        "\n",
        "Signaling the Decoder:\n",
        "The decoder is signaled to start generating the output sequence.\n",
        "Typically, this is done by providing the decoder with an initial input, often a special start token (e.g., <START> or <SOS>).\n",
        "\n",
        "Generating the First Word:\n",
        "For the first word in the output sequence, the decoder combines the following:\n",
        "The start token as the initial query.\n",
        "The encoder's context vectors, which represent the input sequence.\n",
        "The decoder's own context vector for the output sequence (initialized explicitly).\n",
        "These components are used to predict the first word in the output sequence.\n",
        "\n",
        "Subsequent Word Predictions:\n",
        "For generating subsequent words in the output sequence, the following process occurs:\n",
        "The shifted target (previously generated word) becomes the query.\n",
        "The encoder's context vectors, representing the input sequence, are used for context.\n",
        "The context vectors for the target word (which includes context from the encoder) are also considered.\n",
        "The last word's hidden state, obtained from the decoder's self-attention (intra-attention), is incorporated.\n",
        "These components collectively contribute to the prediction of each subsequent word in the output sequence.\n",
        "\n",
        "Iterative Token Generation:\n",
        "The decoder repeats the process of generating tokens one by one, considering context from both the encoder's input sequence and its own generated sequence.\n",
        "At each step, the decoder calculates a probability distribution over the vocabulary for the next token and selects the token with the highest probability.\n",
        "\n",
        "Ending the Sequence:\n",
        "The process continues until the model generates an end token (e.g., <END> or <EOS>) or reaches a predefined maximum sequence length."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hmPY5aUXXORJ"
      },
      "source": [
        "## Transformers Architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dv3t-6btPj-f"
      },
      "source": [
        "<img src=\"https://www.mihaileric.com/static/feedforward_layer_and_normalization-dfdcfbd00009f7f99eca73ae29f2dfb7-4ec3a.png\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NKhhYacdHlku"
      },
      "source": [
        "### Positional Encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "qq2bXmPlHlCJ"
      },
      "outputs": [],
      "source": [
        "def positional_encoding(model_size, SEQUENCE_LENGTH): # d_model\n",
        "  output = []\n",
        "  for pos in range(SEQUENCE_LENGTH):\n",
        "    PE = np.zeros((model_size)) # initilizing with zeros\n",
        "    for i in range(model_size):\n",
        "      if i % 2 == 0: # even positions, sin formula is used according to paper\n",
        "        PE[i] = np.sin(pos/(10000**(i/model_size)))\n",
        "      else: # odd positions, cos formula is used as mentioned in the paper\n",
        "        PE[i] = np.cos(pos/(10000**((i-1)/model_size)))\n",
        "    output.append(tf.expand_dims(PE, axis = 0))\n",
        "\n",
        "  out = tf.concat(output, axis=0)\n",
        "  out = tf.expand_dims(out, axis=0)\n",
        "  return tf.cast(out, dtype=tf.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "KWIb_9ypKJ80",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53ba1476-1b83-4cb0-8703-a5d69f20b0b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 32, 256)\n"
          ]
        }
      ],
      "source": [
        "print(positional_encoding(256, 32).shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RwiXTKylN9Nb"
      },
      "source": [
        "### Input Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "Fb-2bfFuN753"
      },
      "outputs": [],
      "source": [
        "class Embeddings(Layer):\n",
        "  def __init__(self, sequence_length, vocab_size, embedding_dim):\n",
        "    super(Embeddings, self).__init__()\n",
        "    self.token_embeddings = Embedding(input_dim=vocab_size, output_dim=embedding_dim)\n",
        "    self.sequence_length = sequence_length\n",
        "    self.vocab_size = vocab_size\n",
        "    self.embedding_dim = embedding_dim\n",
        "\n",
        "  def call(self, inputs):\n",
        "    embedded_tokens = self.token_embeddings(inputs)\n",
        "    embedded_positions = positional_encoding(self.embedding_dim, self.sequence_length) # PE adding here\n",
        "    return embedded_tokens + embedded_positions # final output for inputs\n",
        "\n",
        "  def compute_mask(self, inputs, mask=None):\n",
        "    return tf.math.not_equal(inputs, 0) # masking function for checking if there are pad tokens(0)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "EzN39sFoQxUy"
      },
      "outputs": [],
      "source": [
        "# testing\n",
        "\n",
        "# test_input = tf.constant([[1, 2, 3, 4, 0, 0, 0]])\n",
        "# embeddings_layer = Embeddings(sequence_length=7, vocab_size=20000, embedding_dim=256)\n",
        "# output_embed = embeddings_layer(test_input)\n",
        "# print(output_embed.shape)\n",
        "# mask = embeddings_layer.compute_mask(test_input)\n",
        "# print(mask)\n",
        "\n",
        "# output: [Batch, Sequence_length, Embedding_dims]\n",
        "# for each and every input there is vector with Embedding dimension 256\n",
        "# for zeros in the input the mask was computed and it was not considered since they are zeros(pad_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "j-V0YD_LktuY"
      },
      "outputs": [],
      "source": [
        "# padding_mask = tf.cast(\n",
        "#     tf.repeat(mask,repeats=tf.shape(mask)[1],axis=0),\n",
        "#     dtype=tf.int32)\n",
        "# print(padding_mask)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Custom Attention Layers\n",
        "\n"
      ],
      "metadata": {
        "id": "se_d-VgdZJxt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Self Attention Layer**\n",
        "<hr>"
      ],
      "metadata": {
        "id": "OelgaGHsZQgz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomSelfAttention(Layer):\n",
        "  def __init__(self, model_size):\n",
        "    super(CustomSelfAttention, self).__init__()\n",
        "    self.model_size = model_size\n",
        "\n",
        "  def call(self, query, key, value, masking):\n",
        "    #### Compute Scores ####\n",
        "    score = tf.matmul(query, key, transpose_b=True)\n",
        "\n",
        "    #### Scaling ####\n",
        "    score = score / tf.math.sqrt(tf.cast(self.model_size, dtype=tf.float32))\n",
        "\n",
        "    #### Masking ####\n",
        "    masking = tf.cast(masking, dtype=tf.float32)\n",
        "    score -= (1.0 - masking) * 1e10\n",
        "\n",
        "    #### Attention Weights ####\n",
        "    attention_weights = tf.nn.softmax(score, axis=-1) * masking\n",
        "\n",
        "    #### Weighted Sum ####\n",
        "    head_output = tf.matmul(attention_weights, value)\n",
        "\n",
        "    #### Output ####\n",
        "    return head_output\n"
      ],
      "metadata": {
        "id": "Uk1jY3YjZOdb"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Multi-Head-Attention Layer**\n",
        "<hr>"
      ],
      "metadata": {
        "id": "WMuAEGjzZYYD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomMultiHeadAttention(Layer):\n",
        "  def __init__(self, num_heads, key_dim):\n",
        "    super(CustomMultiHeadAttention, self).__init__()\n",
        "\n",
        "    self.num_heads = num_heads\n",
        "    self.dense_q = [Dense(key_dim//num_heads) for _ in range(num_heads)]\n",
        "    self.dense_k = [Dense(key_dim//num_heads) for _ in range(num_heads)]\n",
        "    self.dense_v = [Dense(key_dim//num_heads) for _ in range(num_heads)]\n",
        "    self.dense_o = Dense(key_dim)\n",
        "    self.attention = CustomSelfAttention(key_dim)\n",
        "\n",
        "  def call(self, query, key, value, attention_mask):\n",
        "    heads = []\n",
        "\n",
        "    for i in range(self.num_heads):\n",
        "      print('hello', self.dense_q[i](query).shape)\n",
        "      head = self.self_attention(self.dense_q[i](query), self.dense_k[i](key),\n",
        "                                 self.dense_v[i](value), attention_mask)\n",
        "\n",
        "      heads.append(head)\n",
        "    heads = tf.concat(heads, axis=2) # concatenating all heads\n",
        "    heads = self.dense_o(heads) # passing all heads through a linear layer for the final output\n",
        "    return heads\n"
      ],
      "metadata": {
        "id": "5wUWzrZ_avgB"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dqhsy5auXVnN"
      },
      "source": [
        "### Encoder Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "26gOOnLNV2iP"
      },
      "outputs": [],
      "source": [
        "class TransformerEncoder(Layer):\n",
        "  def __init__(self, embedding_dims, dense_dims, num_heads):\n",
        "    super(TransformerEncoder, self).__init__()\n",
        "    self.embedding_dims = embedding_dims\n",
        "    self.dense_dims = dense_dims\n",
        "    self.num_heads = num_heads\n",
        "    self.attention = MultiHeadAttention(num_heads=num_heads, key_dim=embedding_dims)\n",
        "\n",
        "    self.dense_proj = tf.keras.Sequential([\n",
        "        Dense(self.dense_dims, activation=\"relu\"),\n",
        "        Dense(self.embedding_dims),\n",
        "    ])\n",
        "    self.layernorm_1 = LayerNormalization()\n",
        "    self.layernorm_2 = LayerNormalization()\n",
        "    self.supports_masking = True\n",
        "\n",
        "  def call(self, inputs, mask=None):\n",
        "    # print(mask)\n",
        "    if mask is not None:\n",
        "      mask = tf.cast(mask[:, tf.newaxis, :], dtype='int32')\n",
        "      # print(mask)\n",
        "      T = tf.shape(mask)[2]\n",
        "      padding_mask = tf.repeat(mask, T, axis=1)\n",
        "      # print(padding_mask)\n",
        "\n",
        "    attention_output = self.attention(query=inputs, value=inputs, key=inputs, attention_mask=padding_mask)\n",
        "\n",
        "    proj_input = self.layernorm_1(inputs + attention_output)\n",
        "    proj_output = self.dense_proj(proj_input)\n",
        "    return self.layernorm_2(proj_input + proj_output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "xvMJ79aMWUsJ"
      },
      "outputs": [],
      "source": [
        "##### Test for Encoder #####\n",
        "# test_input = tf.random.uniform((1, 10, 256))\n",
        "# test_mask = tf.cast(tf.random.uniform((1, 10)) > 0.5, tf.int32)\n",
        "# encoder = TransformerEncoder(embedding_dims=256, dense_dims=512, num_heads=8)(output_embed)\n",
        "# print(encoder.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mwY8O2xnk-d7"
      },
      "source": [
        "### Decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "UkpG3-lslB3R"
      },
      "outputs": [],
      "source": [
        "class TransformerDecoder(Layer):\n",
        "  def __init__(self, embedding_dims, latent_dims, num_heads):\n",
        "    super(TransformerDecoder, self).__init__()\n",
        "    self.embedding_dims = embedding_dims\n",
        "    self.latent_dims = latent_dims\n",
        "    self.num_heads = num_heads\n",
        "    self.attention_1 = MultiHeadAttention(\n",
        "        num_heads=num_heads, key_dim=embedding_dims\n",
        "    ) # self attention\n",
        "    self.attention_2 = MultiHeadAttention(\n",
        "        num_heads=num_heads, key_dim=embedding_dims\n",
        "    ) # cross-attention with encoder's outputs\n",
        "    self.dense_proj = tf.keras.Sequential(\n",
        "        [Dense(latent_dims, activation='relu'), Dense(embedding_dims)]\n",
        "    ) # feed forward layer\n",
        "    self.layernorm_1 = LayerNormalization() # layer norm for all three layers as in paper\n",
        "    self.layernorm_2 = LayerNormalization()\n",
        "    self.layernorm_3 = LayerNormalization()\n",
        "    self.supports_masking = True # this is special because of decoder\n",
        "\n",
        "  def call (self, inputs, encoder_outputs, enc_mask, mask=None):\n",
        "\n",
        "    if mask is not None:\n",
        "      causal_mask = tf.linalg.band_part(\n",
        "          tf.ones([tf.shape(inputs)[0],\n",
        "                   tf.shape(inputs)[1],\n",
        "                   tf.shape(inputs)[1]], dtype=tf.int32), -1, 0)\n",
        "      # the role of causal mask is to prevent peeking into the future tokens for the decoder to predict better\n",
        "      # the band_part method makes it really easier to do this\n",
        "\n",
        "      mask = tf.cast(\n",
        "          mask[:, tf.newaxis, :], dtype='int32'\n",
        "      )\n",
        "      enc_mask = tf.cast(\n",
        "          enc_mask[:, tf.newaxis, :], dtype='int32'\n",
        "      )\n",
        "\n",
        "      T = tf.shape(mask)[2] # T is the number of queries from the decoder\n",
        "      padding_mask = tf.repeat(mask, T, axis=1)\n",
        "      cross_attn_mask = tf.repeat(enc_mask, T, axis=1)\n",
        "      combined_mask = tf.minimum(padding_mask, causal_mask) # the full mask for the masked mutli-head-connection\n",
        "      # print(f'Padding_mask: {padding_mask}')\n",
        "      # print(f'Causal_mask: {causal_mask}')\n",
        "      # print(f'Combined_mask: {combined_mask}')\n",
        "      # print(f'Cross_attention_mask: {cross_attn_mask}')\n",
        "\n",
        "\n",
        "      attention_output_1 = self.attention_1(\n",
        "          query=inputs, key=inputs, value=inputs,\n",
        "          attention_mask=combined_mask # the first layer which is the self attention for decoder\n",
        "      )\n",
        "\n",
        "      out_1 = self.layernorm_1(inputs + attention_output_1) # the first output + inputs added to be the input\n",
        "      # for the cross_attention layer\n",
        "\n",
        "      attention_output_2 = self.attention_2(\n",
        "          query=out_1, key=encoder_outputs, value=encoder_outputs,\n",
        "          attention_mask=cross_attn_mask # the mask from cross attention just like encoder\n",
        "      )\n",
        "\n",
        "      out_2 = self.layernorm_2(out_1 + attention_output_2) # output 2 after adding and normalizing to be passed\n",
        "      # to feed forward layer for the final outputs\n",
        "\n",
        "      proj_output = self.dense_proj(out_2)\n",
        "\n",
        "      return self.layernorm_3(out_2 + proj_output) # the last norm layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "lf-upalrWnZn"
      },
      "outputs": [],
      "source": [
        "##### Test for Decoder #####\n",
        "# enc_mask = mask\n",
        "# decoder_outputs = TransformerDecoder(embedding_dims=256, latent_dims=512, num_heads=8)(output_embed, encoder, enc_mask)\n",
        "# print(decoder_outputs.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dbOESCXLYxgi"
      },
      "source": [
        "\n",
        "As you see the combined mask does not consider the pad tokens.\n",
        "Lets define each of the mask roles:\n",
        "- Padding mask - removes consideration of unnecessary pad tokens.\n",
        "- Causal mask - Prevents from peeking in the future and helps decoder in predicting one token at a time.\n",
        "- Cross-Attention mask - In the context of cross-attention between the encoder and decoder, a mask is used to ensure that the decoder only attends to positions in the encoder that have valid information. In this case, it can be similar to a padding mask when dealing with sequences of different lengths.\n",
        "- Combined mask - takes the best of both world and  It ensures that the decoder doesn't include padding tokens in its consideration (like the padding mask) and enforces the autoregressive behavior (like the causal mask), allowing the decoder to predict one token at a time while avoiding future tokens.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "9TVhd4prTmtI"
      },
      "outputs": [],
      "source": [
        "# test for band_part method\n",
        "# print(tf.linalg.band_part(tf.ones([10,10], dtype='int32'), -1, 0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D1l831Exqv7s"
      },
      "source": [
        "### Full Transfomers Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "8-ybmslsa6SZ"
      },
      "outputs": [],
      "source": [
        "EMBEDDING_DIMS = 512\n",
        "LATENT_DIMS = 2048\n",
        "NUM_HEADS = 8\n",
        "NUM_LAYERS = 1\n",
        "NUM_EPOCHS = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "mHQAwl21oYHK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7977e4c-c2c8-4656-eed4-b3e074d4b20f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"transformer\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)        [(None, None)]               0         []                            \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)        [(None, None)]               0         []                            \n",
            "                                                                                                  \n",
            " embeddings_2 (Embeddings)   (None, 32, 512)              1024000   ['input_1[0][0]']             \n",
            "                                                          0                                       \n",
            "                                                                                                  \n",
            " embeddings_3 (Embeddings)   (None, 32, 512)              1024000   ['input_2[0][0]']             \n",
            "                                                          0                                       \n",
            "                                                                                                  \n",
            " transformer_encoder_1 (Tra  (None, 32, 512)              1050316   ['embeddings_2[0][0]']        \n",
            " nsformerEncoder)                                         8                                       \n",
            "                                                                                                  \n",
            " tf.math.not_equal_1 (TFOpL  (None, None)                 0         ['input_1[0][0]']             \n",
            " ambda)                                                                                           \n",
            "                                                                                                  \n",
            " transformer_decoder_1 (Tra  (None, 32, 512)              1890560   ['embeddings_3[0][0]',        \n",
            " nsformerDecoder)                                         0          'transformer_encoder_1[0][0]'\n",
            "                                                                    , 'tf.math.not_equal_1[0][0]']\n",
            "                                                                                                  \n",
            " dropout_1 (Dropout)         (None, 32, 512)              0         ['transformer_decoder_1[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " dense_9 (Dense)             (None, 32, 20000)            1026000   ['dropout_1[0][0]']           \n",
            "                                                          0                                       \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 60148768 (229.45 MB)\n",
            "Trainable params: 60148768 (229.45 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "encoder_inputs = Input(shape=(None,), dtype='int64', name='input_1')\n",
        "embeddings = Embeddings(ENGLISH_SEQUENCE_LENGTH, VOCAB_SIZE, EMBEDDING_DIMS)\n",
        "x = embeddings(encoder_inputs)\n",
        "enc_mask = embeddings.compute_mask(encoder_inputs)\n",
        "\n",
        "\n",
        "for _ in range(NUM_LAYERS): # there can be N number of layers as mentioned by paper\n",
        "  x = TransformerEncoder(EMBEDDING_DIMS, LATENT_DIMS, NUM_HEADS)(x)\n",
        "encoder_outputs = x\n",
        "\n",
        "decoder_inputs = Input(shape=(None,), dtype='int64', name='input_2')\n",
        "x = Embeddings(FRENCH_SEQUENCE_LENGTH, VOCAB_SIZE, EMBEDDING_DIMS)(decoder_inputs)\n",
        "\n",
        "for _ in range(NUM_LAYERS):\n",
        "  x = TransformerDecoder(EMBEDDING_DIMS, LATENT_DIMS, NUM_HEADS)(x, encoder_outputs, enc_mask)\n",
        "\n",
        "x = tf.keras.layers.Dropout(0.5)(x)\n",
        "decoder_outputs = Dense(VOCAB_SIZE, activation='softmax')(x)\n",
        "\n",
        "\n",
        "transformer = tf.keras.Model(\n",
        "    [encoder_inputs, decoder_inputs],\n",
        "    decoder_outputs, name='transformer'\n",
        ")\n",
        "\n",
        "transformer.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vNl7CPS0dcUY"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tnXEaY5usBks"
      },
      "source": [
        "## BLEU Metric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "id": "Sn2pQG6frq4E"
      },
      "outputs": [],
      "source": [
        "class BLEU(tf.keras.metrics.Metric):\n",
        "    def __init__(self,name='bleu_score'):\n",
        "        super(BLEU,self).__init__()\n",
        "        self.bleu_score=0\n",
        "\n",
        "    def update_state(self,y_true,y_pred,sample_weight=None):\n",
        "      y_pred=tf.argmax(y_pred,-1)\n",
        "      self.bleu_score=0\n",
        "      for i,j in zip(y_pred,y_true):\n",
        "        tf.autograph.experimental.set_loop_options()\n",
        "\n",
        "        total_words=tf.math.count_nonzero(i)\n",
        "        total_matches=0\n",
        "        for word in i:\n",
        "          if word==0:\n",
        "            break\n",
        "          for q in range(len(j)):\n",
        "            if j[q]==0:\n",
        "              break\n",
        "            if word==j[q]:\n",
        "              total_matches+=1\n",
        "              j=tf.boolean_mask(j,[False if y==q else True for y in range(len(j))])\n",
        "              break\n",
        "\n",
        "        self.bleu_score+=total_matches/total_words\n",
        "\n",
        "    def result(self):\n",
        "        return self.bleu_score/BATCH_SIZE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sgk1TppLdg0J"
      },
      "source": [
        "## Learning Rate Schedular"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "id": "xdVfyVwfdgBn"
      },
      "outputs": [],
      "source": [
        "from keras.optimizers.schedules import LearningRateSchedule\n",
        "\n",
        "class Scheduler(LearningRateSchedule):\n",
        "  def __init__(self, d_model, warmup_steps):\n",
        "    \"\"\" Based on paper \"\"\"\n",
        "    super(Scheduler, self).__init__()\n",
        "    self.d_model = tf.cast(d_model, tf.float64)\n",
        "    self.warmup_steps = tf.cast(warmup_steps, dtype=tf.float64)\n",
        "\n",
        "  def __call__(self, step):\n",
        "    step = tf.cast(step, dtype=tf.float64)\n",
        "    return (self.d_model**(-0.5))*tf.math.minimum(step**(-0.5), step * (self.warmup_steps ** -1.5))\n",
        "    # as the number of steps keeping increasing the Learning Rate keeps decreasing\n",
        "\n",
        "  def get_config(self):\n",
        "      return {\n",
        "          \"d_model\": self.d_model.numpy(),\n",
        "          \"warmup_steps\": self.warmup_steps.numpy()\n",
        "      }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "id": "EtGuY3Cjdoov"
      },
      "outputs": [],
      "source": [
        "WARM_UP_STEPS = 4000\n",
        "lr_scheduled = Scheduler(EMBEDDING_DIMS, WARM_UP_STEPS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "id": "nyfhZadAsGCI"
      },
      "outputs": [],
      "source": [
        "transformer.compile(\n",
        "    optimizer=Adam(lr_scheduled, beta_1=0.9, beta_2=0.98, epsilon=1e-9),\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(),)\n",
        "    # metrics=[BLEU()],\n",
        "    # run_eagerly=True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transformer.optimizer.learning_rate = lr_scheduled # because of config reparams"
      ],
      "metadata": {
        "id": "xQjtf2pv9bKR"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "id": "5da6zVwPsYzq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f528f61d-13d5-478c-c5a8-4054a5b6c81b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "1405/1405 [==============================] - 238s 161ms/step - loss: 5.4360 - val_loss: 4.6908\n",
            "Epoch 2/10\n",
            "1405/1405 [==============================] - 195s 138ms/step - loss: 3.2834 - val_loss: 3.8181\n",
            "Epoch 3/10\n",
            "1405/1405 [==============================] - 192s 137ms/step - loss: 2.6322 - val_loss: 3.5028\n",
            "Epoch 4/10\n",
            "1405/1405 [==============================] - 195s 138ms/step - loss: 2.2687 - val_loss: 3.1521\n",
            "Epoch 5/10\n",
            "1405/1405 [==============================] - 193s 137ms/step - loss: 2.0090 - val_loss: 3.0649\n",
            "Epoch 6/10\n",
            "1405/1405 [==============================] - 193s 137ms/step - loss: 1.8620 - val_loss: 3.0069\n",
            "Epoch 7/10\n",
            "1405/1405 [==============================] - 194s 137ms/step - loss: 1.7258 - val_loss: 2.9751\n",
            "Epoch 8/10\n",
            "1405/1405 [==============================] - 193s 137ms/step - loss: 1.6314 - val_loss: 2.9634\n",
            "Epoch 9/10\n",
            "1405/1405 [==============================] - 193s 137ms/step - loss: 1.5566 - val_loss: 2.9341\n",
            "Epoch 10/10\n",
            "1405/1405 [==============================] - 193s 137ms/step - loss: 1.5016 - val_loss: 3.0033\n"
          ]
        }
      ],
      "source": [
        "history = transformer.fit(\n",
        "    train_dataset,\n",
        "    epochs=10,\n",
        "    validation_data=val_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transformer.save('transformer.hdf5')"
      ],
      "metadata": {
        "id": "dco-2a-QfMUS"
      },
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MgsxM6FRu7Uy"
      },
      "source": [
        "# Testing and Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model loss plots"
      ],
      "metadata": {
        "id": "a6RaruE8sDYa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "id": "ne1Lhnn9mTPW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "outputId": "c6e183eb-a064-4543-9b5e-d6d54bf16b3e"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model Loss and Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend(['Train', 'Val'])\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Evaluation"
      ],
      "metadata": {
        "id": "0K4qP_j2sHVQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from tensorflow.keras.models import load_model\n",
        "# from tensorflow.keras.utils import custom_object_scope\n",
        "\n",
        "# with custom_object_scope({'Embeddings': Embeddings,\n",
        "#                           'TransformerEncoder': TransformerEncoder,\n",
        "#                           'TransformerDecoder': TransformerDecoder}):\n",
        "#     transformer = load_model('transformer.hdf5')\n",
        "\n"
      ],
      "metadata": {
        "id": "CzldWzDNsJ7a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Translating texts"
      ],
      "metadata": {
        "id": "YWnogdqpsQwI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "index_to_word={x:y for x, y in zip(range(len(french_vectorize_layer.get_vocabulary())),\n",
        "                                   french_vectorize_layer.get_vocabulary())}"
      ],
      "metadata": {
        "id": "oxoRUXDKsQRI"
      },
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def translator(english_sentence):\n",
        "  tokenized_english_sentence=english_vectorize_layer([english_sentence])\n",
        "  shifted_target='starttoken'\n",
        "\n",
        "  for i in range(FRENCH_SEQUENCE_LENGTH):\n",
        "    tokenized_shifted_target=french_vectorize_layer([shifted_target])\n",
        "    output=transformer.predict([tokenized_english_sentence,tokenized_shifted_target])\n",
        "    french_word_index=tf.argmax(output,axis=-1)[0][i].numpy()\n",
        "    current_word=index_to_word[french_word_index]\n",
        "    if current_word=='endtoken':\n",
        "      break\n",
        "    shifted_target+=' '+current_word\n",
        "  return shifted_target[11:]"
      ],
      "metadata": {
        "id": "TwDd4fXFsX-7"
      },
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "translator('What makes you think that it is not true?')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        },
        "id": "frcen8q5sZes",
        "outputId": "ac13798c-3ba1-4165-e44a-189cd32c39ae"
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 3s 3s/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'questce qui vous fait penser que ce nest pas vrai'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "translator('Have you ever watched soccer under the rain?')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 137
        },
        "id": "6I-rCugashGv",
        "outputId": "2fbb319e-3191-4d9c-a1dc-680f9e66b02f"
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'astu déjà regardé le [UNK]'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 110
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "translator(\"what is your name?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 171
        },
        "id": "Vuff5O02sh7l",
        "outputId": "88b5b0d3-1f33-4cb7-a9fb-00baebf417e6"
      },
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'ton nom est ton nom de famille'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 111
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "translator('Great trees do not grow with ease, the stronger the winds, the stronger the trees')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        },
        "id": "kNLbe2pcskcj",
        "outputId": "742677c1-3e40-42dd-fce0-a37d1b08bc54"
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'de nombreuses arbres ne sapplique pas avec un fort fort'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 112
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Its not bad considering less training"
      ],
      "metadata": {
        "id": "AWjNGrn2-2KD"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "yHoYbLZ0Vd1Z"
      ],
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "V100",
      "mount_file_id": "1bU7GhF23xWvpCyve3p8sX7aqbg235nCJ",
      "authorship_tag": "ABX9TyPEkn6ByR483/jx87yvTPVZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}